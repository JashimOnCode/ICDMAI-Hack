{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfa81d2-3e14-450c-9c5c-b19c7b81ab44",
   "metadata": {},
   "source": [
    "Making of Datasets , one for model training and another for data streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb6dde2-f28a-4583-8ccd-b314643ab0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "    Customer_ID   Age  Tenure  Monthly_Usage  Complaints  Returns  \\\n",
      "0            1  56.0      29      69.302250           3        0   \n",
      "1            2  69.0      44     215.284417           4        2   \n",
      "2            3  46.0      53     378.077243           1        2   \n",
      "3            4  32.0      24      44.793756           0        0   \n",
      "4            5  60.0      58      49.288207           3        1   \n",
      "\n",
      "   Emails_Opened  Daily_Logins  Sensor_Triggers Event_Timestamp  Churn  \n",
      "0            9.0             2                9      2024-01-25      0  \n",
      "1            0.0             1                2      2024-05-18      1  \n",
      "2            7.0             1                3      2024-01-17      0  \n",
      "3            3.0             3                0      2024-02-05      0  \n",
      "4            8.0             2                3      2024-04-05      0  \n",
      "\n",
      "Sample Event Stream Data:\n",
      "    Customer_ID Event_Timestamp      Event_Type  Event_Value\n",
      "0          812      2024-02-27           login            2\n",
      "1          838      2024-01-04           login            6\n",
      "2          603      2024-06-21      email_open            9\n",
      "3         1187      2024-03-06  sensor_trigger            7\n",
      "4          318      2024-04-24  sensor_trigger            4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Simulate sample data\n",
    "np.random.seed(42)\n",
    "num_rows = 2000\n",
    "start_date = datetime.datetime(2024, 1, 1)\n",
    "\n",
    "data = {\n",
    "    'Customer_ID': range(1, num_rows + 1),\n",
    "    'Age': np.random.randint(18, 70, num_rows),\n",
    "    'Tenure': np.random.randint(1, 60, num_rows),  # in months\n",
    "    'Monthly_Usage': np.random.uniform(10, 500, num_rows),\n",
    "    'Complaints': np.random.randint(0, 5, num_rows),\n",
    "    'Returns': np.random.randint(0, 3, num_rows),\n",
    "    'Emails_Opened': np.random.randint(0, 10, num_rows),\n",
    "    'Daily_Logins': np.random.randint(0, 5, num_rows),\n",
    "    'Sensor_Triggers': np.random.randint(0, 10, num_rows),\n",
    "    'Event_Timestamp': [start_date + datetime.timedelta(days=np.random.randint(0, 180)) for _ in range(num_rows)],\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Simulate churn with correlations\n",
    "df['Churn'] = df.apply(lambda row: \n",
    "    1 if (row['Complaints'] > 3) or (row['Monthly_Usage'] < 50 and np.random.rand() > 0.5) else 0, axis=1)\n",
    "\n",
    "# Introduce missing values\n",
    "for col in ['Age', 'Monthly_Usage', 'Emails_Opened']:\n",
    "    df.loc[df.sample(frac=0.05).index, col] = np.nan\n",
    "\n",
    "# Save main customer dataset\n",
    "df.to_csv('customer_data_large.csv', index=False)\n",
    "print(\"Sample Data:\\n\", df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688350a-a0e4-43a9-b76b-4e4ff0d26674",
   "metadata": {},
   "source": [
    "Making a Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688bb2e2-b73c-4335-a6f5-b0af108fd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the pre-generated customer data CSV\n",
    "df = pd.read_csv('customer_data_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f5cc06-9a9c-459e-9a0f-a27338c7e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer_ID   Age  Tenure  Monthly_Usage  Complaints  Returns  \\\n",
      "0            1  56.0      29      69.302250           3        0   \n",
      "1            2  69.0      44     215.284417           4        2   \n",
      "2            3  46.0      53     378.077243           1        2   \n",
      "3            4  32.0      24      44.793756           0        0   \n",
      "4            5  60.0      58      49.288207           3        1   \n",
      "\n",
      "   Emails_Opened  Daily_Logins  Sensor_Triggers Event_Timestamp  Churn  \n",
      "0            9.0             2                9      25-01-2024      0  \n",
      "1            0.0             1                2      18-05-2024      1  \n",
      "2            7.0             1                3      17-01-2024      0  \n",
      "3            3.0             3                0      05-02-2024      0  \n",
      "4            8.0             2                3      05-04-2024      0  \n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows to verify the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610aabd7-ce61-4930-90f1-605ba6f96b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Handling missing values with mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['Age', 'Monthly_Usage', 'Emails_Opened']] = imputer.fit_transform(df[['Age', 'Monthly_Usage', 'Emails_Opened']])\n",
    "\n",
    "# Feature Selection: Drop non-numeric or irrelevant features (like Customer_ID and Event_Timestamp)\n",
    "df = df.drop(columns=['Customer_ID', 'Event_Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82381d3-d2af-4ef1-bd37-33803cd06e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling: Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df[['Age', 'Tenure', 'Monthly_Usage', 'Complaints', 'Returns', 'Emails_Opened', 'Daily_Logins', 'Sensor_Triggers']] = scaler.fit_transform(\n",
    "    df[['Age', 'Tenure', 'Monthly_Usage', 'Complaints', 'Returns', 'Emails_Opened', 'Daily_Logins', 'Sensor_Triggers']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60126ac3-a945-4a9c-890e-32a43b169360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% training, 20% testing)\n",
    "X = df.drop(columns='Churn')  # Features\n",
    "y = df['Churn']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4f9600-2674-40f2-8201-683e8565ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training: Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f752b60-ae02-4f8f-8582-d772dc61d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "623d48f1-14a1-438e-b251-8da731271a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation: Predictions and Performance Metrics\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91a6c0d8-4728-4c47-a9a2-f63f801b4d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       305\n",
      "           1       0.85      0.85      0.85        95\n",
      "\n",
      "    accuracy                           0.93       400\n",
      "   macro avg       0.90      0.90      0.90       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "Confusion Matrix:\n",
      " [[291  14]\n",
      " [ 14  81]]\n"
     ]
    }
   ],
   "source": [
    "# Print out classification metrics\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9ecc2a0-8289-41a8-9295-a913beb737f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/churn_prediction_model.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model \n",
    "joblib.dump(model, 'models/churn_prediction_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8894c-3505-481d-861c-36c1152fba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
